<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="Small mini blog sized notes, or note sized blogs.">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Notes</title>
  <link rel="stylesheet" href="css/main.css">

</head>
<body>
  <div class="container">
    <h1><a href="./">Notes</a></h1>
    <p><a href="#deconv">Deconvolution Layer</a><br />
<a href="#batchnorm">Batch Normalization</a><br />
<a href="#qlearningsarsa">Q-learning v SARSA</a><br />
<a href="#policyvalue">Policy Iteration v Value Iteration</a> <br />
<a href="#qlearning">Q Learning</a><br />
<a href="#policygrad">Policy Gradients</a><br />
<a href="#actorcritic">Actor Critic methods</a><br />
<a href="#trpo">Trust Region Methods</a><br />
<a href="#mcts">Monte Carlo Tree Search</a><br />
<a href="#irl">Inverse Reinforcement Learning</a><br />
<a href="#oneshot">One shot learning</a><br />
<a href="#meta">Meta learning</a><br />
<a href="#a3c">A3C</a><br />
<a href="#ddl">Distributed DL</a></p>

<hr />

<h2 id="deconvolution-layer"><a name="deconv"></a>Deconvolution Layer</h2>

<ul>
  <li>torch.nn.ConvTranspose2d in PyTorch</li>
  <li>ambiguous name, no deconvolutions</li>
  <li>a deconvolution layer maps from a lower to higher dimension, a sort of upsampling</li>
  <li>the transpose of a non-padded convolution is equivalent to convolving a zero-padded input</li>
  <li>zeroes are inserted between inputs which cause the kernel to move slower, hence also called fractionally strided convolution</li>
  <li>deconv layers allow the model to use every point in the small image to “paint” a square in the larger one</li>
  <li>deconv layers have uneven overlap in the output, conv layers have overlap in the input</li>
  <li>leads to the problem of checkerboard artifacts</li>
  <li>resize-convolution instead transposed-convolution to avoid checkerboard artifacts</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html" target="_blank">Convolution Arithmatic</a></li>
  <li><a href="https://distill.pub/2016/deconv-checkerboard/">Distil Blog Post</a></li>
  <li><a href="http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf">Original Paper</a></li>
</ul>

<hr />

<h2 id="batch-normalization"><a name="batchnorm"></a>Batch Normalization</h2>

<ul>
  <li>torch.nn.BatchNorm2d in PyTorch</li>
  <li>normalizses the data in each batch to have zero mean and unit covariance</li>
  <li>provides some consistency between layers by reducing internal covariate shift</li>
  <li>allows a higher learning rate to be used, reduces the learning time</li>
  <li>after normalizing the input, it is squased through a linear function with parameters gamma and beta</li>
  <li>output of batchnorm = gamma * normalized_input + beta</li>
  <li>having gamma and beta allows the network to choose how much ‘normalization’ it wants for every feature; shift and scale</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://www.youtube.com/watch?v=gYpoJMlgyXA&amp;feature=youtu.be&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&amp;t=3078">Andrej Karapathy’s lecture</a></li>
  <li><a href="https://arxiv.org/abs/1502.03167">Original Paper</a></li>
  <li><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">Read this later</a></li>
</ul>

<hr />

<h2 id="q-learning-v-sarsa"><a name="qlearningsarsa"></a>Q-learning v SARSA</h2>

<ul>
  <li>SARSA stands for state-action-reward-state-action</li>
  <li>SARSA is on-policy; that is sticks to the policy it is learning. Q-learning is off-policy</li>
  <li>SARSA improves the estimate of Q by using the transitions from the policy dervied from Q</li>
  <li>Q-learning updates the Q estimate using the observed reward and the maximum reward possible <script type="math/tex">max_a{a\prime} Q(s\prime, a\prime)</script> for the next state</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html">Pseudo Codes</a></li>
  <li><a href="https://stackoverflow.com/questions/32846262/q-learning-vs-sarsa-with-greedy-select">StackOverFlow</a></li>
</ul>

<hr />

<h2 id="policy-iteration-v-value-iteration"><a name="policyvalue"></a>Policy Iteration v Value Iteration</h2>

<ul>
  <li>PI: trying to converge the policy to optimal; VI: trying to converge the value function to optimal</li>
  <li>PI: policy evaluation (calculating value function using <script type="math/tex">v(s) \gets \sum_{s\prime} p(s\prime \mid s, \pi (s)) [r(s, \pi (s), s\prime) + \gamma v(s\prime)]</script>) ) + policy improvement; repeat until policy is stable</li>
  <li>VI: policy evaluation (calculating value function using <script type="math/tex">v(s) \gets max_a \sum_{s\prime} p(s\prime \mid s,a) [r(s,a,s\prime) + \gamma v(s\prime)]</script>); single policy update</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration">StackOverFlow</a></li>
</ul>

<hr />

<h2 id="q-learning"><a name="qlearning"></a>Q Learning</h2>

<ul>
  <li>Model free learning: the agent has no idea about the state transition and reward functions; it learns everything from experience by interacting with the environment</li>
  <li>Q-Learning is based on Time-Difference Learning</li>
  <li>
    <script type="math/tex; mode=display">Q(s_t, a_t) = Q(s_t, a_t) + \alpha[r(s,a) + \gamma * max_a Q(s_{t+1}, a) - Q(s_t, a_t)]</script>
  </li>
  <li>See notes on Q-Learning v SARSA</li>
  <li><script type="math/tex">\epsilon</script>-greedy approach: choose a random action with probability <script type="math/tex">\epsilon</script>, or action according to the current estimate of Q-values otherwise; this approach controls the exploration vs exploitation</li>
</ul>

<p>References</p>
<ul>
  <li>Sutton Book 1st Ed Page 148</li>
  <li><a href="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa">Medium post</a></li>
</ul>

<hr />
<h2 id="policy-gradients"><a name="policygrad"></a>Policy Gradients</h2>

<ul>
  <li>Run a policy for a while; see what actions led to higher rewards; increase their probability</li>
  <li>Take the gradient of log probability of trajectory, then weight it by the final reward</li>
  <li>Increase the probability of actions that lead to higher reward</li>
  <li>With <script type="math/tex">J(\theta)</script> as the policy objective function
<script type="math/tex">\nabla_{\theta} J(\theta) = \sum_{t \geq 0} r(\tau) \;\nabla_{\theta} \;log\; \pi_{\theta} (a_t \mid s_t)</script></li>
  <li>This suffers from high variance and is a simplistic view; credit assignment problem is hard</li>
  <li>Baseline: whether a reward is better or worse than what you expect to get</li>
  <li>A simple baseline: constant moving average of rewards experienced so far from all trajectories; Vanilla REINFORCE</li>
  <li>Reducing variance further using better baselines -&gt; Actor critic algorithm</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf">CS231n RL Lecture</a></li>
  <li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf">David Silver slides</a></li>
</ul>

<hr />

<h2 id="actor-critic-methods"><a name="actorcritic"></a>Actor Critic Methods</h2>

<ul>
  <li>Works well when there is an infinite input and output space</li>
  <li>Requires much less training time than policy gradient methods</li>
  <li>Actor =&gt; takes in the environment states and determines the best action to take</li>
  <li>Critic =&gt; takes in the environment and the action from the actor and returns a score that represents how good the action is for the state</li>
  <li>Both the actor (policy) and critic (Q function) are different neural networks</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf">CS231n RL Lecture</a></li>
  <li><a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_5_actor_critic_pdf.pdf">CS294 DeepRL</a></li>
</ul>

<hr />

<h2 id="trust-region-methods"><a name="trpo"></a>Trust Region Methods</h2>

<ul>
  <li>A kind of local policy search algorithm</li>
  <li>‘local’ because every new policy is somewhat closer to the earlier policy</li>
  <li>TRPO uses policy gradients but has a constraint on how the polices are updated</li>
  <li>Each new policy has to be close to the older one in terms of the KL-divergence</li>
  <li>Since polices are nothing but probability distributions over the actions, KL divergence is a natural way to measure the distance</li>
  <li>Constraint Policy Optimization (CPO) is another trust region method using contraints on the cost function to keep an agent’s action under a limit while maintaining optimal performance</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/abs/1502.05477">TRPO paper</a></li>
  <li><a href="http://bair.berkeley.edu/blog/2017/07/06/cpo/">OpenAI Blog on CPO</a></li>
</ul>

<hr />

<h2 id="monte-carlo-tree-search"><a name="mcts"></a>Monte Carlo Tree Search</h2>

<ul>
  <li>MCTS is based on two idea:
    <ul>
      <li>a true value of an action may be evaluated using random simulation</li>
      <li>these values maybe used to efficiently adjust the policy towards a best-first strategy</li>
    </ul>
  </li>
  <li>THe algorithm builds a search tree till a computational budget - time or memory is exhausted</li>
  <li>The algorthim has four parts which are applied per iteration
    <ul>
      <li>Selection: descending down the root node till an expandable non-terminal node</li>
      <li>Expansion: adding child nodes towards the tree</li>
      <li>Simulation: simulate the default policy from the new node(s) to produce an output</li>
      <li>Backpropagation: the simulation result is ‘backed up’ through the selected nodes</li>
    </ul>
  </li>
  <li>Selection + Expansion =&gt; Tree policy; Simluation =&gt; Default policy</li>
  <li>The backpropagatin step informs future tree policy decision</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://gnunet.org/sites/default/files/Browne%20et%20al%20-%20A%20survey%20of%20MCTS%20methods.pdf">MCTS Survey paper</a></li>
  <li>Sutton 2nd Edition 8.11 Page 153</li>
</ul>

<hr />

<h2 id="inverse-reinforcement-learning"><a name="irl"></a>Inverse Reinforcement Learning</h2>

<ul>
  <li>Learning the reward fucntion by observing expert behaviour</li>
  <li>Imitation learning (behaviour cloning and IRL) tries to copy the teacher’s actions</li>
  <li>Learning the reward function can make the system robust to changes in the environment’s transition mechanics</li>
  <li>Learning the reward function is also transferable from one type of agent to another, as it encodes all that is needed to excel in the envirnment</li>
  <li>Think of IRL as a way to learn an abstraction or latent representation of the target</li>
  <li>Another big motivation for IRL is that it is extremely difficult to manually specifiy a reward function to an agent, like in a self driving car</li>
  <li>Instead of simply copying the expert behavior, we can then try to learn the underlying reward function which the expert is trying to optimize</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html">Blog post</a></li>
</ul>

<hr />

<h2 id="one-shot-imitation-learning"><a name="oneshot"></a>One Shot Imitation Learning</h2>

<ul>
  <li>Trying to learn with very limited demonstrations</li>
  <li>The model is given multiple demonstrations and conditioned on one instance of a task, to help learn that task, and so on similarly other tasks as well</li>
  <li>Generalise the understanding of various tasks</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1703.07326.pdf">Paper</a></li>
</ul>

<hr />

<h2 id="meta-learning"><a name="meta"></a>Meta Learning</h2>

<ul>
  <li>The agent learns a policy to learn policies</li>
  <li>Given a task and an model, the agent can learn a policy to master that task</li>
  <li>But it may fail if the task is altered</li>
  <li>Meta Learning tries to devise methods to learn policies which can learn policies further and can therefore perform multiple tasks</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/abs/1611.05763">Learning to reinforcement learn</a></li>
  <li><a href="https://arxiv.org/abs/1611.02779">RL^2</a></li>
</ul>

<hr />

<h2 id="asynchronous-actor-critic-agents-a3c"><a name="a3c"></a>Asynchronous Actor-Critic Agents (A3C)</h2>

<ul>
  <li>Asychronous Advantage Actor-Critic</li>
  <li>Asychronous: Unlike other learning agent algos like DQN, A3C has multiple worker agents interacting with the environment providing a more diverse experience to the learning phase</li>
  <li>Advantage: like in PG methods</li>
  <li>Actor-Critic: same as <a href="#actorcritic">Actor Critic</a></li>
  <li>The workers independently work by learning from the environment and update the global network</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1602.01783.pdf">Paper</a></li>
</ul>

<hr />

<h2 id="distributed-dl"><a name="ddl"></a>Distributed DL</h2>

<ul>
  <li><strong>Synchronous Distributed SGD (Centralised)</strong>
    <ul>
      <li>Parameter Server</li>
      <li>Gradients are sent to the parameter server that computes the updates</li>
      <li>Workers reeive updated models</li>
    </ul>
  </li>
  <li><strong>Synchronous Distributed SGD (Decentralised)</strong>
    <ul>
      <li>All-Reduce the gradients to every worker</li>
      <li>Models on each node are updated with the same average gradients</li>
    </ul>
  </li>
  <li><strong>Asynchronous Distributed SGD (Centralised)</strong>
    <ul>
      <li>Asynchronous parameter udpates</li>
      <li>Lag problem</li>
      <li>Workers update when they complete their gradient calculation</li>
    </ul>
  </li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1802.09941.pdf">Survey paper on Distributed DL</a></li>
</ul>

<hr />

  </div>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</body>
</html>

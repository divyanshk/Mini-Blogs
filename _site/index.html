<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="description" content="Small mini blog sized notes, or note sized blogs.">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Notes</title>
  <link rel="stylesheet" href="css/main.css">

</head>
<body>
  <div class="container">
    <h1><a href="./">Notes</a></h1>
    <p><a href="#deconv">Deconvolution Layer</a><br />
<a href="#batchnorm">Batch Normalization</a><br />
<a href="#qlearningsarsa">Q-learning v SARSA</a><br />
<a href="#policyvalue">Policy Iteration v Value Iteration</a> <br />
<a href="#qlearning">Q Learning</a><br />
<a href="#policygrad">Policy Gradients</a><br />
<a href="#actorcritic">Actor Critic methods</a><br />
<a href="#trpo">Trust Region Methods</a><br />
<a href="#mcts">Monte Carlo Tree Search</a><br />
<a href="#irl">Inverse Reinforcement Learning</a><br />
<a href="#oneshot">One shot learning</a><br />
<a href="#meta">Meta learning</a><br />
<a href="#a3c">A3C</a><br />
<a href="#ddl">Distributed DL</a><br />
<a href="#mac">MAC vs Digital Signatures</a><br />
<a href="#mle">MLE and KL Divergence</a><br />
<a href="#lips">Lipschitz Continuity</a> <br />
<a href="#bias">Exposure bias problem</a> <br />
<a href="#gini">Gini coefficient</a>  <br />
<a href="#pareto">Pareto distribution</a></p>

<hr />

<h2 id="deconvolution-layer"><a name="deconv"></a>Deconvolution Layer</h2>

<ul>
  <li>torch.nn.ConvTranspose2d in PyTorch</li>
  <li>ambiguous name, no deconvolutions</li>
  <li>a deconvolution layer maps from a lower to higher dimension, a sort of upsampling</li>
  <li>the transpose of a non-padded convolution is equivalent to convolving a zero-padded input</li>
  <li>zeroes are inserted between inputs which cause the kernel to move slower, hence also called fractionally strided convolution</li>
  <li>deconv layers allow the model to use every point in the small image to “paint” a square in the larger one</li>
  <li>deconv layers have uneven overlap in the output, conv layers have overlap in the input</li>
  <li>leads to the problem of checkerboard artifacts</li>
  <li>resize-convolution instead transposed-convolution to avoid checkerboard artifacts</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://deeplearning.net/software/theano_versions/dev/tutorial/conv_arithmetic.html" target="_blank">Convolution Arithmatic</a></li>
  <li><a href="https://distill.pub/2016/deconv-checkerboard/">Distil Blog Post</a></li>
  <li><a href="http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf">Original Paper</a></li>
</ul>

<hr />

<h2 id="batch-normalization"><a name="batchnorm"></a>Batch Normalization</h2>

<ul>
  <li>torch.nn.BatchNorm2d in PyTorch</li>
  <li>normalizses the data in each batch to have zero mean and unit covariance</li>
  <li>provides some consistency between layers by reducing internal covariate shift</li>
  <li>allows a higher learning rate to be used, reduces the learning time</li>
  <li>after normalizing the input, it is squased through a linear function with parameters gamma and beta</li>
  <li>output of batchnorm = gamma * normalized_input + beta</li>
  <li>having gamma and beta allows the network to choose how much ‘normalization’ it wants for every feature; shift and scale</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://www.youtube.com/watch?v=gYpoJMlgyXA&amp;feature=youtu.be&amp;list=PLkt2uSq6rBVctENoVBg1TpCC7OQi31AlC&amp;t=3078">Andrej Karapathy’s lecture</a></li>
  <li><a href="https://arxiv.org/abs/1502.03167">Original Paper</a></li>
  <li><a href="https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html">Read this later</a></li>
</ul>

<hr />

<h2 id="q-learning-v-sarsa"><a name="qlearningsarsa"></a>Q-learning v SARSA</h2>

<ul>
  <li>SARSA stands for state-action-reward-state-action</li>
  <li>SARSA is on-policy; that is sticks to the policy it is learning. Q-learning is off-policy</li>
  <li>SARSA improves the estimate of Q by using the transitions from the policy dervied from Q</li>
  <li>Q-learning updates the Q estimate using the observed reward and the maximum reward possible <script type="math/tex">max_a{a\prime} Q(s\prime, a\prime)</script> for the next state</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html">Pseudo Codes</a></li>
  <li><a href="https://stackoverflow.com/questions/32846262/q-learning-vs-sarsa-with-greedy-select">StackOverFlow</a></li>
</ul>

<hr />

<h2 id="policy-iteration-v-value-iteration"><a name="policyvalue"></a>Policy Iteration v Value Iteration</h2>

<ul>
  <li>PI: trying to converge the policy to optimal; VI: trying to converge the value function to optimal</li>
  <li>PI: policy evaluation (calculating value function using <script type="math/tex">v(s) \gets \sum_{s\prime} p(s\prime \mid s, \pi (s)) [r(s, \pi (s), s\prime) + \gamma v(s\prime)]</script>) ) + policy improvement; repeat until policy is stable</li>
  <li>VI: policy evaluation (calculating value function using <script type="math/tex">v(s) \gets max_a \sum_{s\prime} p(s\prime \mid s,a) [r(s,a,s\prime) + \gamma v(s\prime)]</script>); single policy update</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://stackoverflow.com/questions/37370015/what-is-the-difference-between-value-iteration-and-policy-iteration">StackOverFlow</a></li>
</ul>

<hr />

<h2 id="q-learning"><a name="qlearning"></a>Q Learning</h2>

<ul>
  <li>Model free learning: the agent has no idea about the state transition and reward functions; it learns everything from experience by interacting with the environment</li>
  <li>Q-Learning is based on Time-Difference Learning</li>
  <li>
    <script type="math/tex; mode=display">Q(s_t, a_t) = Q(s_t, a_t) + \alpha[r(s,a) + \gamma * max_a Q(s_{t+1}, a) - Q(s_t, a_t)]</script>
  </li>
  <li>See notes on Q-Learning v SARSA</li>
  <li><script type="math/tex">\epsilon</script>-greedy approach: choose a random action with probability <script type="math/tex">\epsilon</script>, or action according to the current estimate of Q-values otherwise; this approach controls the exploration vs exploitation</li>
</ul>

<p>References</p>
<ul>
  <li>Sutton Book 1st Ed Page 148</li>
  <li><a href="https://medium.com/@m.alzantot/deep-reinforcement-learning-demysitifed-episode-2-policy-iteration-value-iteration-and-q-978f9e89ddaa">Medium post</a></li>
</ul>

<hr />
<h2 id="policy-gradients"><a name="policygrad"></a>Policy Gradients</h2>

<ul>
  <li>Run a policy for a while; see what actions led to higher rewards; increase their probability</li>
  <li>Take the gradient of log probability of trajectory, then weight it by the final reward</li>
  <li>Increase the probability of actions that lead to higher reward</li>
  <li>With <script type="math/tex">J(\theta)</script> as the policy objective function
<script type="math/tex">\nabla_{\theta} J(\theta) = \sum_{t \geq 0} r(\tau) \;\nabla_{\theta} \;log\; \pi_{\theta} (a_t \mid s_t)</script></li>
  <li>This suffers from high variance and is a simplistic view; credit assignment problem is hard</li>
  <li>Baseline: whether a reward is better or worse than what you expect to get</li>
  <li>A simple baseline: constant moving average of rewards experienced so far from all trajectories; Vanilla REINFORCE</li>
  <li>Reducing variance further using better baselines -&gt; Actor critic algorithm</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf">CS231n RL Lecture</a></li>
  <li><a href="http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf">David Silver slides</a></li>
</ul>

<hr />

<h2 id="actor-critic-methods"><a name="actorcritic"></a>Actor Critic Methods</h2>

<ul>
  <li>Works well when there is an infinite input and output space</li>
  <li>Requires much less training time than policy gradient methods</li>
  <li>Actor =&gt; takes in the environment states and determines the best action to take</li>
  <li>Critic =&gt; takes in the environment and the action from the actor and returns a score that represents how good the action is for the state</li>
  <li>Both the actor (policy) and critic (Q function) are different neural networks</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture14.pdf">CS231n RL Lecture</a></li>
  <li><a href="http://rll.berkeley.edu/deeprlcourse/f17docs/lecture_5_actor_critic_pdf.pdf">CS294 DeepRL</a></li>
</ul>

<hr />

<h2 id="trust-region-methods"><a name="trpo"></a>Trust Region Methods</h2>

<ul>
  <li>A kind of local policy search algorithm</li>
  <li>‘local’ because every new policy is somewhat closer to the earlier policy</li>
  <li>TRPO uses policy gradients but has a constraint on how the polices are updated</li>
  <li>Each new policy has to be close to the older one in terms of the KL-divergence</li>
  <li>Since polices are nothing but probability distributions over the actions, KL divergence is a natural way to measure the distance</li>
  <li>Constraint Policy Optimization (CPO) is another trust region method using contraints on the cost function to keep an agent’s action under a limit while maintaining optimal performance</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/abs/1502.05477">TRPO paper</a></li>
  <li><a href="http://bair.berkeley.edu/blog/2017/07/06/cpo/">OpenAI Blog on CPO</a></li>
</ul>

<hr />

<h2 id="monte-carlo-tree-search"><a name="mcts"></a>Monte Carlo Tree Search</h2>

<ul>
  <li>MCTS is based on two idea:
    <ul>
      <li>a true value of an action may be evaluated using random simulation</li>
      <li>these values maybe used to efficiently adjust the policy towards a best-first strategy</li>
    </ul>
  </li>
  <li>THe algorithm builds a search tree till a computational budget - time or memory is exhausted</li>
  <li>The algorthim has four parts which are applied per iteration
    <ul>
      <li>Selection: descending down the root node till an expandable non-terminal node</li>
      <li>Expansion: adding child nodes towards the tree</li>
      <li>Simulation: simulate the default policy from the new node(s) to produce an output</li>
      <li>Backpropagation: the simulation result is ‘backed up’ through the selected nodes</li>
    </ul>
  </li>
  <li>Selection + Expansion =&gt; Tree policy; Simluation =&gt; Default policy</li>
  <li>The backpropagatin step informs future tree policy decision</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://gnunet.org/sites/default/files/Browne%20et%20al%20-%20A%20survey%20of%20MCTS%20methods.pdf">MCTS Survey paper</a></li>
  <li>Sutton 2nd Edition 8.11 Page 153</li>
</ul>

<hr />

<h2 id="inverse-reinforcement-learning"><a name="irl"></a>Inverse Reinforcement Learning</h2>

<ul>
  <li>Learning the reward fucntion by observing expert behaviour</li>
  <li>Imitation learning (behaviour cloning and IRL) tries to copy the teacher’s actions</li>
  <li>Learning the reward function can make the system robust to changes in the environment’s transition mechanics</li>
  <li>Learning the reward function is also transferable from one type of agent to another, as it encodes all that is needed to excel in the envirnment</li>
  <li>Think of IRL as a way to learn an abstraction or latent representation of the target</li>
  <li>Another big motivation for IRL is that it is extremely difficult to manually specifiy a reward function to an agent, like in a self driving car</li>
  <li>Instead of simply copying the expert behavior, we can then try to learn the underlying reward function which the expert is trying to optimize</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html">Blog post</a></li>
</ul>

<hr />

<h2 id="one-shot-imitation-learning"><a name="oneshot"></a>One Shot Imitation Learning</h2>

<ul>
  <li>Trying to learn with very limited demonstrations</li>
  <li>The model is given multiple demonstrations and conditioned on one instance of a task, to help learn that task, and so on similarly other tasks as well</li>
  <li>Generalise the understanding of various tasks</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1703.07326.pdf">Paper</a></li>
</ul>

<hr />

<h2 id="meta-learning"><a name="meta"></a>Meta Learning</h2>

<ul>
  <li>The agent learns a policy to learn policies</li>
  <li>Given a task and an model, the agent can learn a policy to master that task</li>
  <li>But it may fail if the task is altered</li>
  <li>Meta Learning tries to devise methods to learn policies which can learn policies further and can therefore perform multiple tasks</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/abs/1611.05763">Learning to reinforcement learn</a></li>
  <li><a href="https://arxiv.org/abs/1611.02779">RL^2</a></li>
</ul>

<hr />

<h2 id="asynchronous-actor-critic-agents-a3c"><a name="a3c"></a>Asynchronous Actor-Critic Agents (A3C)</h2>

<ul>
  <li>Asychronous Advantage Actor-Critic</li>
  <li>Asychronous: Unlike other learning agent algos like DQN, A3C has multiple worker agents interacting with the environment providing a more diverse experience to the learning phase</li>
  <li>Advantage: like in PG methods</li>
  <li>Actor-Critic: same as <a href="#actorcritic">Actor Critic</a></li>
  <li>The workers independently work by learning from the environment and update the global network</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1602.01783.pdf">Paper</a></li>
</ul>

<hr />

<h2 id="distributed-dl"><a name="ddl"></a>Distributed DL</h2>

<ul>
  <li><strong>Synchronous Distributed SGD (Centralised)</strong>
    <ul>
      <li>Parameter Server</li>
      <li>Gradients are sent to the parameter server that computes the updates</li>
      <li>Workers reeive updated models</li>
    </ul>
  </li>
  <li><strong>Synchronous Distributed SGD (Decentralised)</strong>
    <ul>
      <li>All-Reduce the gradients to every worker</li>
      <li>Models on each node are updated with the same average gradients</li>
    </ul>
  </li>
  <li><strong>Asynchronous Distributed SGD (Centralised)</strong>
    <ul>
      <li>Asynchronous parameter udpates</li>
      <li>Lag problem</li>
      <li>Workers update when they complete their gradient calculation</li>
    </ul>
  </li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1802.09941.pdf">Survey paper on Distributed DL</a></li>
</ul>

<hr />

<h2 id="mac-vs-digital-signatures"><a name="mac"></a>MAC vs Digital Signatures</h2>

<ul>
  <li>Message Authentication Codes (MAC): detects modification of messages based on a ‘shared key’
    <ul>
      <li>Symmetric key based algorithms for pretecting integrity</li>
      <li>Example: HMAC (key-hashed MAC), CBC-MAC / CMAC (block cipher based)</li>
    </ul>
  </li>
  <li>Digital Signatures: detects modification of messages based on a asymmetric key pair
    <ul>
      <li>Asymmetric keys: public key and private key</li>
      <li>The sender signs with its private key, the receiver can verify the signature with the sender’s public key</li>
    </ul>
  </li>
  <li>MACs are faster and take less size; but Digital Signatures provide non-repudiation (if the recipient passes the message and the proof to a third party, can the third party be confident that the message originated from the sender ?)</li>
</ul>

<p>References</p>
<ul>
  <li><a href="http://www.cs.cornell.edu/courses/cs5430/2016sp/l/08-macdigsig/notes.html">Cornell Course page</a></li>
  <li><a href="https://crypto.stackexchange.com/questions/6523/what-is-the-difference-between-mac-and-hmac">Stackexchange</a></li>
</ul>

<hr />

<h2 id="mle-and-kl-divergence"><a name="mle"></a>MLE and KL Divergence</h2>

<ul>
  <li>Maximum likelihood estimation (MLE):
Given a dataset <script type="math/tex">\mathcal{D}</script> of sie n drawn from a distribution <script type="math/tex">P_{\theta} \in \mathcal{P}</script>, the MLE estimate of <script type="math/tex">\theta</script> is defined as</li>
</ul>

<p><script type="math/tex">\hat{\theta} = arg\;max_{\theta} \; \mathcal{L}(\theta, D)</script> or 
<script type="math/tex">\hat{\theta} = arg\;max_{\theta} \; log \; P_{\theta}(\mathcal{D}|\theta)</script></p>
<ul>
  <li>Equivalently, this can be formulated as iid samples, negative log-likelihood</li>
</ul>

<p><script type="math/tex">NLL(\theta) = - \sum^N_{i=1} log \; p(y_i|x_i, \theta)</script></p>
<ul>
  <li>KL Divergence: 
Relative entropy, it measures the dissimilarity of two probability distributions</li>
</ul>

<p><script type="math/tex">\mathcal{KL} (p||q) = \sum^K_{k=1} \; p_k \; log \; \frac{p_k}{q_k}</script></p>
<ul>
  <li>Expand above to get <script type="math/tex">\mathcal{KL} = -\mathcal{H}(p) + \mathcal{H}(p,q)</script></li>
  <li>In the limit, KL is same as MLE</li>
  <li>In generative models, MLE isn’t suitable as the probability density under the trained model for any actual input is almost always zero</li>
  <li>For more details: <a href="https://www.inference.vc/how-to-train-your-generative-models-why-generative-adversarial-networks-work-so-well-2/">blog</a></li>
</ul>

<p>References</p>
<ul>
  <li>Murphy’s book</li>
  <li><a href="https://wiseodd.github.io/techblog/2017/01/26/kl-mle/">Blog</a></li>
  <li><a href="https://stats.stackexchange.com/a/345138">StackExchange</a></li>
</ul>

<hr />

<h2 id="lipschitz-continuity"><a name="lips"></a>Lipschitz Continuity</h2>

<ul>
  <li>This property is often used in deep learning and differential equations over ‘funny’ functions</li>
  <li>Lipschitz continuity is a simple way to bound the function values <br />
<script type="math/tex">|f(x) - f(y)| \leq K \ |x-y|</script></li>
  <li>Refer to the wiki page for a more generalized defination</li>
  <li>Notice, the Lipschitz constant <script type="math/tex">K</script> is the bound on the slope AKA derivative of the function in the specified domain</li>
  <li>Using this condition provides a safe way to talk about differentiability of the function (Rademacher’s Theorem)</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://math.stackexchange.com/questions/353276/intuitive-idea-of-the-lipschitz-function">StackExchange</a></li>
  <li><a href="https://www.intfxdx.com/downloads/rademacher-thm-2015.pdf">Theorem</a></li>
  <li><a href="https://users.wpi.edu/~walker/MA500/HANDOUTS/LipschitzContinuity.pdf">Note</a></li>
</ul>

<hr />

<h2 id="exposure-bias-problem"><a name="bias"></a>Exposure bias problem</h2>

<ul>
  <li>Recurrent models are trained to predict the next word given the previous ground truth words as input</li>
  <li>At test time, they are used to generate an entire sequence by predicting one word at a time, and by feeding the generated word back as input at the next time
step</li>
  <li>This is not good because the model was trained on a different distribution of inputs, namely, words drawn from the data distribution, as opposed to words drawn from the model distribution</li>
  <li>The errors made along the way will quickly accumulate</li>
  <li>This is knowns as exposure bias which occurs when a model is only exposed to the training data distribution, instead of its own predictions</li>
  <li>This is the discrepancy between training and inference stages</li>
</ul>

<p>References</p>
<ul>
  <li><a href="https://arxiv.org/pdf/1511.06732.pdf">Paper</a></li>
</ul>

<hr />

<h2 id="gini-coefficient"><a name="gini"></a>Gini Coefficient</h2>

<ul>
  <li>Gini coefficient is a single number aimed at measureing the degree of inequality in a distribution.</li>
  <li>Given a group of people producing posts/comments, this can be used to estimate the dispersion in content production, i.e., most posts/comments come from a selectd few or from a diverse set of users.</li>
  <li>A gini coefficient of 0 means perfect equality, and 1 means perfect concentration in a single individual.</li>
  <li>
    <script type="math/tex; mode=display">G = \frac{\sum_{i=1}^{n}\sum_{n}{j=1}}|x_i-x_j|}{2n^2\hat{x}}</script>
  </li>
  <li>where n is the number of participating members, and <script type="math/tex">x_i</script> is the content produced, or wealth.</li>
  <li>Alternatively, Gini coefficient can be thought of as the ratio of the area that lies between the line of equality and the Lorenz curve over the total area under the line of equality.</li>
  <li>Points on the Lorenz curve is the proportion of overall income or wealth assumed by the botton x% of the people [economics]. See the income distribution graph on the Lorenz curve wiki page.</li>
</ul>

<hr />

<h2 id="pareto-distribution"><a name="pareto"></a>Pareto distribution</h2>

<ul>
  <li>lorem</li>
  <li>ipsum</li>
</ul>

<hr />

  </div>
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

</body>
</html>
